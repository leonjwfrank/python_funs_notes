## 神经网络主要结构
    生物神经元结构:树突，轴突，突触
    人工神经元: 输入层，隐藏层，输出层(全连接层)
    
    神经网络主要目的是对给定对一组数据，找到其映射关系函数f，得到f(x)=wx+b中常量w，b的值,计算预测值。
    基础单元为神经元（感知机），每个神经元都与下一层相接所做的事是线性变换或非线性变换，输入向量，输出标量。
    从结构上可分为：输入层，隐藏层，输出层，比如一个神经网络有3层，一般包括输入层1，隐藏层2，输出层1。
    从参数结构类型可分为：DNN（深度神经网络），CNN（卷积神经网络），RNN（循环神经网络），每一种网络都有其对应的参数，常用激活函数。
    
    全连接层 将所有特征融合到一起。
    池化层 减少训练参数，是对原始特征信号进行采样。
    
    当输入数据过大时，卷积层计算量很大，为了减少参数，因此有了池化层。
    激活层  激活函数，Relu， 神经元两种状态，兴奋，抑制，事实上处于不同程度'兴奋'对神经元传播化学物质也不尽相同。
    非线性激活层就是决定哪些神经元对活跃程度高，哪些神经元活跃程度低
    
    特征提取:
        9 的特征  0 + 1
        8 的特征  0 + 0
        4 的特征  1 + - + 1
    评价预测结果是否好
        损失函数，代价函数，评估神经网络训练结果与真实值的差距，越小越接近真实结果。
        数字识别的损失函数 不适合用 归类 的损失函数不是非0 即 1的，需要[0,1]区间损失函数
        平方损失函数
        预测值与真实值的差值
    
    前馈型神经网络
        输入 -- 隐藏 --- 输出
    反馈型神经网络
        各层之间有环
        
## 基本概念模块
    神经元和神经网络
        前馈神经网络，线性回归，数字识别
    基于神经网络的深度学习模块
        网络构建
        计算损失函数
        优化器函数
        反向传播
     总结:
     训练过程就是最小化损失函数的过程
     通过损失函数来定量计算神经网络预测结果的好坏
     损失函数减小说预测结果越接近真实结果(标签)
     
     优化器
     如果得到来一个次优解，想得到最优解，需要不断调整优化器。
     梯度下降算法 是最有效的算法，也可以辅助其他优化算法。
     
     反向传播  Backpropagation
        从输出层向输入层传播
        一个神经元到输入输出可以类比为一个多元一次线性方程，二元一次  y = w1x1 + w2x2 + b
        
     
        
## 深度学习优势    
    1、优化方法的变革是开启深度学习复兴之门的钥匙
    特别是无监督的、分层预训练多层神经网络
    2，从经验驱动的人造特征范式到数据驱动的表示学习范式
    3、从“分步、分治”到“端到端的学习”
      分步或分治法将复杂问题分解为若干简单子问题和步骤，是解决复杂问题的步骤，同样适用于AI
      子问题或子步骤变得简单、可控，更易解决。
      这样的问题是
      但从深度学习的视角来看，其劣势也是同样明显的：子问题最优未必意味着全局的最优，每个子步骤是最优的也不意味着全过程来看是最优的      
    4，端到端学习的优势
       深度学习更强调端到端的学习（end-to-end learning），即：不去人为的分步骤或者划分子问题，而是完全交给神经网络直接学习从原始输入到期望输出的映射。
       相比分治策略，端到端的学习具有协同增效（synergy）的优势，有更大的可能获得全局上更优的解
       分层看成是“子步骤或子问题”也是可以的，但这些分层各自完成什么功能并不是我们预先设定好的，而是通过基于数据的全局优化来自动学习的
    5，深度学习具备超强的非线性建模能力
        深度学习实现了从输入到输出的非线性变换，这是深度学习在众多复杂问题上取得突破的重要原因之一
        深度学习则通过作用于大量神经元的非线性激活函数（如Sigmoid或ReLU），获得了可以适配足够复杂的非线性变换的能力。
    6，奥卡姆剃刀原理在诸多领域特别是机器学习领域广为人知，它告诫人们：“如无必要，勿增实体”。
    换句话说，求解问题的模型能简单最好不要复杂。
    提高模型推广能力的重要法则，也使得复杂的大模型往往不被看好。而深度学习恰恰在这一点上是令人费解的，以AlexNet为例，其需要学习的参数（权重）多达6000万个，如此之巨的参数似乎表明这是一个非常复杂（如果不是过分复杂的话）的模型
    7，脑神经科学启发的思路值得更多的重视
        深度学习作为多层神经网络是受脑神经科学的启发而发展起来的
    特别是卷积神经网络
        该模型的提出动机就是模拟哺乳动物视觉神经系统的感受野逐渐变大、逐层提取由简及繁的特征，从而实现语义逐级抽象的视觉神经通路
    
    8，尚未建模的问题
    生物神经系统的连接极为复杂，不仅仅有自下而上的前馈和同层递归，更有大量的自上而下的反馈，以及来自其他神经子系统的外部连接，这些都是目前的深度模型尚未建模的。
    特异化的神经细胞可能也会处理其他的感觉请求
    视觉皮层的大量神经细胞在失去视觉处理需求后不久，即被“重塑”转而处理触觉或其他模态的数据。神经系统的这种可塑性意味着不同的智能处理任务具有良好的通用性，为通用人工智能的发展提供了参照
    
    深度学习对无监督数据的学习能力严重不足
    深度学习以“数据驱动”范式颠覆了“人造特征”范式，这是一个重大的进步。但与此同时，它自己又陷入了一个“人造结构”窠臼中
    计算的角度来看，全面的学习网络结构是极其复杂的
    
    这意味着先验知识、上下文、猜测和想象（脑补）等“智能”能力难以在现有深度网络上得到应用和体现。
    
    而演绎推理在很多时候似乎与数据无关
    基于大数据的深度学习可以认为是一种归纳法，而从一般原理出发进行演绎是人类的另一重要能力，特别是在认知和决策过程中，我们大量依赖演绎推理
    
    9，当前深度学习分类
    当前深度学习领域的学术研究可以包含四部分：
    优化（Optimization），
        深度学习的问题最后似乎总能变成优化问题，这个时候数值优化的方法就变得尤其重要
    泛化（Generalization）
        个模型的泛化能力是指它在训练数据集上的误差是否能够接近所有可能测试数据误差的均值。泛化误差大致可以理解成测试数据集误差和训练数据集误差之差。
        在深度学习领域变流行之前，如何控制泛化误差一直是机器学习领域的主流问题。
    表达（Representation）
        这方面主要指的是深度学习模型和它要解决的问题之间的关系，比如给出一个设计好的深度学习模型，它适合表达什么样的问题，以及给定一个问题是否存在一个可以进行表达的深度学习模型。
    应用（Applications）
        深度学习的发展伴随着它对其它领域的革命过程。在过去的数年中，深度学习的应用能力几乎是一种“敢想就能成”的状态。这当然得益于现今各行各业丰富的数据集以及计算机计算能力的提升，同时也要归功于过去近三十年的领域经验。未来，深度学习将继续解决各种识别（Recognition）相关的问题，比如视觉（图像分类、分割，计算摄影学），语音（语音识别），自然语言（文本理解）；同时，在能够演绎（Ability to Act）的方面如图像文字描述、语音合成、自动翻译、段落总结等也会逐渐出现突破，更可能协助寻找NP难（NP-Hard）问题在限定输入集之后的可行算法
    实际工作内容:
    数据处理
    1. 深度网络压缩：目的是将权值参数进行量化或者压缩存储，进而减少参数规模。
    2. 模型加速：现在出现了更深的网络，但带来的问题是计算效率的下降。这个方向主要从网络架构和实现的角度对模型的计算效率进行提升。
    3. 优化：现在在ICML上边关于DL优化的文章很多，如何避免 overfiting? 如何加速训练？产生初始参数？这些都是比较热门的研究点。
    4. 应用。包括检测、分割、人脸、NLP等，一个好的工作会综合考虑各种各样的因素。
    5. 迁移。在CV领域work的模型是否可以应用到其他领域？在一个新领域中基本想法有了，但具体做起来需要解决各种各样的实际问题

        
## 线性空间和模型
    以判别式降维为目的的线性子空间方法得到大家的重视，如主成分分析，Fisher线性判别分析，独立成分分析等
    为了处理非线性问题，Kernel技巧、流形学习等非线性处理方法相继得到重视。其中Kernel方法试图实现对原始输入的非线性变换，但却无法定义显式的非线性变换，只能借助有限种类的kernel函数，定义目标空间中的点积，间接实现非线性。
    
               
## 常见激活函数
    深度神经网络激活函数包括以下几种：
    Sigmod函数：特征相差不大的情况下适合使用该方法，将一个实数映射到(0,1)区间，但是在分布比较平滑的地方，梯度容易消失。
    Tanh函数：取值范围(-1,1)，tanh(x)=sinh(x）/ cosh(x），tanh函数有比sigmoid更快的收敛速度，从而降低来迭代所需次数。
    ReLU函数：大于0的输入数据可正常计算，但是小于0的部分可能被省略，造成更大误差。
    softmax函数：多分类问题每一个可能性都计算出来，它将多个神经元的输出映射到(0,1)空间，用于多分类问题，每一个分类的可能性都可以计算出来。
    
## DNN 算法主要步骤？
    正向传播: 得到参数确认参数的过程 常量w，b的值,计算预测值
    反向传播: 求偏导，求的损失函数
    梯度下降: 更新参数，迭代更新参数，得到最小损失函数，接近真实值的为最优解。
    
    
## 深度学习主要应用场景
    深度学习具有优秀的自动提取特征的能力，能够学习多层次的抽象特征表示，并对异质或跨域的内容信息进行学习
    图像识别
        基于深度学习的图像分类方法，通过有监督无监督的方法学习层次化的特征描述，从而取得手工设计或选择图像特征的工作，比如效果较好的CNN直接使用
        图像像素信息作为输入，最大程度保留图像的信息，通过卷积操作进行特征提取和高层操作，模型输出直接是图像识别结果。这就是输入-输出，端到端的学习方法效果很好。
        基础任务
            图形分类
            物体跟踪，行为分析，目标检测，图像分割
        数字识别
        文字识别
   
    自然语言处理
        应用: 搜索引擎，广告系统，推荐系统
        词向量:
            我们经常要比较两个词或者两段文本之间的相关性。
            向量空间模型(vector space model)。 
                在这种模型里，每个词被表示成一个实数向量（one-hot vector），其长度为字典大小，每个维度对应一个字典里的每个词，除了这个词对应维度上的值是1，其他元素都是0。
            每个词本身的信息量很小。所以，仅仅给定两个词，不足以让我们准确判别它们是否相关。
            要想精确计算相关性，我们还需要更多的信息——从大量数据里通过机器学习方法归纳出来的知识
        
        词向量模型:
            词向量模型可以是概率模型、共生矩阵(co-occurrence matrix)模型或神经元网络模型。在用神经网络求词向量之前，传统做法是统计一个词语的共生矩阵X。
            X是一个|V|×|V| 大小的矩阵，Xij表示在所有语料中，词汇表V(vocabulary)中第i个词和第j个词同时出现的词数，|V|为词汇表的大小。
            对X做矩阵分解（如奇异值分解，Singular Value Decomposition [5]）
            但这样的传统做法有很多问题：

            由于很多词没有出现，导致矩阵极其稀疏，因此需要对词频做额外处理来达到好的矩阵分解效果；
            矩阵非常大，维度太高(通常达到106×106的数量级)；
            需要手动去掉停用词（如although, a,...），不然这些频繁出现的词也会影响矩阵分解的效果。
            基于神经网络的模型不需要计算和存储一个在全语料上统计产生的大表，而是通过学习语义信息得到词向量，因此能很好地解决以上问题
        基于神经网络的词向量模型
            语言模型旨在为语句的联合概率函数P(w1,...,wT)建模, 其中wi表示句子中的第i个词。语言模型的目标是，希望模型对有意义的句子赋予大概率，对没意义的句子赋予小概率。 
            这样的模型可以应用于很多领域，如机器翻译、语音识别、信息检索、词性标注、手写识别等，它们都希望能得到一个连续序列的概率。 
            以信息检索为例，当你在搜索“how long is a football bame”时（bame是一个医学名词），搜索引擎会提示你是否希望搜索"how long is a football game", 
            这是因为根据语言模型计算出“how long is a football bame”的概率很低，而与bame近似的，可能引起错误的词中，game会使该句生成的概率最大。

            对语言模型的目标概率P(w1,...,wT)，如果假设文本中每个词都是相互独立的，则整句话的联合概率可以表示为其中所有词语条件概率的乘积
            N-gram neural model
                n-gram 是文本语言模型的重要表示方法
                n-gram模型也是统计语言模型中的一种重要方法，用n-gram训练语言模型时，一般用每个n-gram的历史n-1个词语组成的内容来预测第n个词。
                神经概率语言模型（Neural Network Language Model，NNLM）通过一个线性映射和一个非线性隐层连接，同时学习了语言模型和词向量，即通过学习大量语料得到词语的向量表达，通过这些向量得到整个句子的概率
                
                
            Continuous Bag-of-Words model(CBOW)
                CBOW模型通过一个词的上下文（各N个词）预测当前词。 
                
            Skip-gram model
                CBOW的好处是对上下文词语的分布在词向量上进行了平滑，去掉了噪声，因此在小数据集上很有效。而Skip-gram的方法中，用一个词预测其上下文，
                得到了当前词上下文的很多样本，因此可用于更大的数据集
                
    个性化推荐(Recommender System)
        信息超载: 技术不断发展和电子商务数量和种类快速增长，用户需要大量时间才能找到自己想要的商品
        个性化推荐系统可以解决以上问题，不需要用户准确地描述出自己的需求，而是根据用户的历史行为进行建模，主动提供满足用户兴趣和需求的信息
        主要方法:
            协同过滤推荐（Collaborative Filtering Recommendation）：该方法是应用最广泛的技术之一，需要收集和分析用户的历史行为、活动和偏好
                它不依赖于机器去分析物品的内容特征，因此它无需理解物品本身也能够准确地推荐诸如电影之类的复杂物品；缺点是对于没有任何行为的新用户存在冷启动的问题，同时也存在用户与商品之间的交互数据不够多造成的稀疏问题。值得一提的是，社交网络[3]或地理位置等上下文信息都可以结合到协同过滤中去。
            基于内容过滤推荐
                该方法利用商品的内容描述，抽象出有意义的特征，通过计算用户的兴趣和商品描述之间的相似度，来给用户做推荐。优点是简单直接，不需要依据其他用户对商品的评价，而是通过商品属性进行商品相似度度量，从而推荐给用户所感兴趣商品的相似商品；缺点是对于没有任何行为的新用户同样存在冷启动的问题
            组合推荐
                运用不同的输入和技术共同进行推荐，以弥补各自推荐技术的缺点
                
                
    情感分析
        情感分析一般是指判断一段文本所表达的情绪状态。其中，一段文本可以是一个句子，一个段落或一个文档。情绪状态可以是两类，
        如（正面，负面），（高兴，悲伤）；也可以是三类，如（积极，消极，中性）等等。
        经典文本表示方法:
            词袋模型BOW(bag of words)
                在一段文本，BOW表示会忽略其词顺序、语法和句法，将这段文本仅仅看做是一个词集合，因此BOW方法并不能充分表示文本的语义信息。
            话题模型
        分类方法
            支持向量机 SVM(support vector machine)
            逻辑回归 LR(logistic regression)
        
        
        神经网络
            使用卷积处理输入的词向量序列，产生一个特征图(feature map)，
            对特征图采用时间维度的最大池化(max pooling over time)得到此卷积对应的整句话的特征
            最后将所有卷积核得到的特征拼接起来即为文本的定长向量表示，对于文本分类问题，将其连接至softmax即构建出完整的模型
            
        循环神经网络RNN
            循环神经网络（RNN）
            循环神经网络是一种能对序列数据进行精确建模的有力工具。实际上，循环神经网络的理论计算能力是图灵完备的[4]。
            自然语言是一种典型的序列数据（词序列），近年来，循环神经网络及其变体（如long short term memory[5]等）在自然语言处理的多个领域，
            如语言模型、句法解析、语义角色标注（或一般的序列标注）、语义表示、图文生成、对话、机器翻译等任务上均表现优异甚至成为目前效果最好的方法
        
        长短期记忆网络LSTM(long short term memory)
            为解决循环神经网络训练过程中容易出现的梯度消失或梯度爆炸的问题而提出
            LSTM增加了记忆单元c、输入门i、遗忘门f及输出门o。这些门及记忆单元组合起来大大提升了循环神经网络处理长序列数据的能力。若将基于LSTM的循环神经网络表示的函数记为 F
        
        栈式双向LSTM（Stacked Bidirectional LSTM）
            正常顺序的循环神经网络，ht包含了t时刻之前的输入信息，也就是上文信息。同样，为了得到下文信息，我们可以使用反方向（将输入逆序处理）的循环神经网络。结合构建深层循环神经网络的方法（深层神经网络往往能得到更抽象和高级的特征表示），我们可以通过构建更加强有力的基于LSTM的栈式双向循环神经网络[9]，来对时序数据进行建模
            
    语义角色标注
        词法分析、句法分析和语义分析是NLP的三大层面
        完全句法分析需要确定句子所包含的全部句法信息，并确定句子各成分之间的关系，是一个非常困难的任务，目前技术下的句法分析准确率并不高，句法分析的细微错误都会导致SRL的错误
        语义角色是指论动词所指事件中担任的角色。
        主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等
        语义角色标注（Semantic Role Labeling，SRL）以句子的谓词为中心，不对句子所包含的语义信息进行深入分析，只分析句子中各成分与谓词之间的关系，
        即句子的谓词（Predicate）- 论元（Argument）结构，并用语义角色来描述这些结构关系，是许多自然语言理解任务（如信息抽取，篇章分析，深度问答等）的一个重要中间步骤。在研究中一般都假定谓词是给定的，所要做的就是找出给定谓词的各个论元和它们的语义角色。
        传统的SRL系统大多建立在句法分析基础之上，通常包括5个流程：

        构建一棵句法分析树
        从句法树上识别出给定谓词的候选论元。
        候选论元剪除；一个句子中的候选论元可能很多，候选论元剪除就是从大量的候选项中剪除那些最不可能成为论元的候选项。
        论元识别：这个过程是从上一步剪除之后的候选中判断哪些是真正的论元，通常当做一个二分类问题来解决。
        对第4步的结果，通过多分类得到论元的语义角色标签。可以看到，句法分析是基础，并且后续步骤常常会构造的一些人工特征，这些特征往往也来自句法分析
        
        模型:
            栈式循环神经网络（Stacked Recurrent Neural Network
            双向循环神经网络（Bidirectional Recurrent Neural Network）
            条件随机场 (Conditional Random Field)
            深度双向LSTM（DB-LSTM）SRL模型
    
    机器翻译
        GRU
        循环神经网络（RNN）及长短时间记忆网络（LSTM）。相比于简单的RNN，LSTM增加了记忆单元（memory cell）、输入门（input gate）、遗忘门（forget gate）及输出门（output gate）
        这些门及记忆单元组合起来大大提升了RNN处理远距离依赖问题的能力。

        GRU[2]是Cho等人在LSTM上提出的简化版本，也是RNN的一种扩展，如下图所示。GRU单元只有两个门：

        重置门（reset gate）：如果重置门关闭，会忽略掉历史信息，即历史不相干的信息不会影响未来的输出。
        更新门（update gate）：将LSTM的输入门和遗忘门合并，用于控制历史信息对当前时刻隐层输出的影响。如果更新门接近1，会把历史信息传递下去。
        
        双向循环神经网络
        
        编码器-解码器框架
        
        注意力机制
        
        柱搜索算法
        
## 生成对抗网络（Generative Adversarial Network，简称GAN）
     非监督式学习的一种方法，通过让两个神经网络相互博弈的方式进行学习
     生成对抗网络由一个生成网络与一个判别网络组成。生成网络从潜在空间（latent space）中随机采样作为输入，其输出结果需要尽量模仿训练集中的真实样本。
     判别网络的输入为真实样本或生成网络的输出，其目的是将生成网络的输出从真实样本中尽可能分辨出来。而生成网络则要尽可能地欺骗判别网络。
     两个网络相互对抗、不断调整参数，其目的是将生成网络生成的样本和真实样本尽可能的区分
     
     GAN 网络顾名思义，是一种通过对抗的方式，去学习数据分布的生成模型。其中，“对抗”指的是生成网络（Generator)和判别网络（Discriminator)的相互对抗。这里以生成图片为例进行说明：

     生成网络（G）接收一个随机的噪声z，尽可能的生成近似样本的图像，记为G(z)
     判别网络（D）接收一张输入图片x，尽可以去判别该图像是真实样本还是网络生成的假样本，判别网络的输出 D(x) 代表 x 为真实图片的概率。
     如果 D(x)=1 说明判别网络认为该输入一定是真实图片，如果 D(x)=0 说明判别网络认为该输入一定是假图片。

     应用:
        生成以假乱真的图片。
        生成视频
        三维物体模型
     
     模型:
        GAN
        DCGAN
            是深层卷积网络与 GAN 的结合，其基本原理与 GAN 相同，只是将生成网络和判别网络用两个卷积网络（CNN）替代。为了提高生成样本的质量和网络的收敛速度

## 强化学习
    目标: 求得最佳策略
        强化学习不关心当前状态分为什么类别，而是关心是否执行最佳动作
        状态值函数V
            只和状态相关，用于对某个局面状态进行估值
        状态动作函数Q - Q_learning
            与状态以及在该状态下采取对动作相关，用于对某个局面状态下采取对某个动作进行估值
        强化学习中常用对算法 Q-Learning
            选择估值最好的动作，然后执行
        简单Q函数表  Q-Table
            Q函数表中表示状态，列表示动作，表中的值表示特定状态下执行某动作的评估值Q
            主体通过不断更新并查找该表，找到当前状态回报最高的动作执行。
            总是需要 找到一张行动指南表
        某个时候 策略的Q函数表 Q-Table
        * 一只老鼠和奶酪的游戏
        状态/动作   上   下   左   右
           开始点   0    20  0   10
        一小块奶酪   0  -100  1   2
        空白        0   100  10   0
        两小块奶酪   5    0   0   -100
          毒药      0    0   0    0
          一堆奶酪   0    0   0   0
            
    实例
        在游戏领域如围棋等 AlphaGo
        通过不断试错 和 纠正来学习到正确路径。
        有实验用计算机做一个小游戏，用机器学习到方法，通过一个步骤，计算机使用了 150万次，但是通过第二个步骤就大大缩减了，在经过一段时间的训练，就大大超过了人类。
        
        强化学习在gym 游戏
        https://github.com/openai/gym
        