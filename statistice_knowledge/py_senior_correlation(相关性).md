# 分布

## 抽样分布

# 相关 correlation
    1，概念
    标准差
        方差的平方根
    方差
        描述数据序列的离散度，sigma**2 = 1/n * E(1,n)(xi - u)**2
    卡方统计量
        X**2 = 𝜮(i) ((O_i - E_i) **2) /E_i
    协方差
        两个或多个参数 变化趋势是否相同
        dx_i = xi-ux   # ux为X的均值
        dy_i = yi-uy   # uy为Y的均值
        将x和y的离差相乘，二者符号相同时，乘积为正，这些乘积的加和 可以用来衡量两个序列变化是否一致
        协方差就是这些乘积结果的平均值
            Cov(X,Y) = (1/n)∑dx_idy_i
        这里n表示序列的长度，X和Y必须有相同长度。但是这个值很难解释，很难说明有什么意义
    离差
        序列每个元素 与均值的差
    标准分数
        序列的离差 除以 标准差
        
### 皮尔逊相关系数
    
    研究某两个参数之间的关系时，有以下问题
        1，一般两个参数有不同的度量衡，如 重量为 kg， 身高为 cm
        2，多个参数 的各自的分布也不同
    有序列lis1=[x1,x2,x3...], lis2=[a1,a2,a3...]
    解决以上问题，可以使用以下方法
        1，标准分数sandard score， 皮尔逊相关系数
            序列的每一个数据元素的标准分数: z_i = (x_i - u_i) / sigma
            数据元素 - 均值 除以标准差
        2，将所有的值转换为百分等级(也可以是序列的秩)，这就是斯皮尔曼相关系数
    
    问题:
    1,皮尔逊相关系数(Pearson's correlation)
    单位为 1， 取值范围 (-1， 1)
    缺点:皮尔逊系数 对异常值很敏感
    
    序列A = [x1,x2,x3...], B = [a1,a2,a3...]
    两个序列的标准分数乘积 P_i= (x_i- u_x)* (a_i - u_y) / (sigma_x * sigma_y)
    这些乘积均值为:
        p_i = (1/n) ∑ P_i
    改写一下这个公式的形式
        p_i = Cov(X,Y)/(sigma_x, sigma_y)
    将离差项代入公司，可得
        p_i =  ∑(dx_idyi)/ ∑dx_i ∑dy_i
    利用柯西-施瓦兹不等式(Cauchy-Schwarz inequality)，可以证明以上 p_i**2 <=1, 故而 -1 <= p_i <= 1
    p_i 的绝对值大小代表两个变量的相关程度，两个变量完全相关，即如果我们知道了其中一个变量的值，可以精确的预测另一个变量的值，
    p_i = -1 时也是同义的情况，只是两个变量完全负相关而已,大部分情况并不是完全相关或 不相关的
    
    皮尔逊系数描述的是两个参数的线性相关性，而且相关系数根斜率是无关的，所以
    当皮尔逊系数 p = 0，不能说明两个变量之间毫无关系，两个参数有可能是非线性相关的
    
    为了解决非线性相关，我们在计算相关系数之前，一定要画散点图观察一下数据
    
    3，一个优秀的散点图
        散点图可以探测两个变量是否具有相关性的简单方法
        扰动 jitter
            当搜集数据时，可能数据被取整导致信息丢失，在分析的时候，比如画散点图，可以加一个扰动
            如身高，体重相关的计算时，扰动可以是 [-0.5,0.5]或[-1.3,1.3]范围的均匀分布随机数
        重叠
            当数据很多时，可以加一个透明度参数 a 来解决这个问题
            当有重叠时，散点因为是透明的，重叠的地方颜色会更浓
        巨量的数据
            当数据量大的时候，可以使用称为hexbin的方法
            1，将 散点图 分成一个个小格子，统计每个格子中有多少个数据点，
            pyplot.scatter(heights,weights,cmap = matplotlib.cm.Blues)
            2，然后根据格子中的点的个数上色
            3，这个图的好处是能展示数据关系的整体形状，对于大型数据非常高效
            缺点：无法了解异常值
     
       
### 斯皮尔曼秩相关 spearman's Rank Correlation
    在处理序列时，将序列转换成秩之后，再计算皮尔逊相关系数，得到的结果就是斯皮尔曼秩相关系数
    
    另一个方法: 对原始数据做一个变化，使得变换之后的结果接近正态分布，然后再算皮尔逊相关系数
    例如: 如果数据近似服从对数正态分布，那么我们可以先对数据取对数，然后再算相关系数
    
    设一个数据序列为 {7,1,2,5} 秩转换后为 {4,1,2,3}
    如果数据序列中出现多个相同的值，那么严格的做法是给这些数值赋予一个秩的平均值，如果不这么严格，而是随意给这些值安排个顺序，一般也不会造成什么误差
    1，编写函数计算序列的秩，
    2，编写计算两个数据序列的斯皮尔曼秩相关的系数
    参考: thinkstats.com/correlation.py
   
    参考: 请绘制体重和身高的斯皮尔曼秩相关系数. 你觉得哪个相关系数能更好地描述两个变量关系的强度?
     thinkstats.com/brfss_corr.py
     读懂BFRSS数据的代码，生成散点图，参考   brfss.py, brfss_scatter.py    

### 最小二乘拟合
    最小相关可以衡量两个变量之间线性相关的强度和正负，
    缺点是无法知道他们的斜率
    
    1，估计斜率
    有很多方法可以用来 **估计斜率**
    
    其中 **线性最小二乘拟合** linear least square fit 是最常用的一种方法
    线性拟合 linear fit指的是用一个线性方程拟合两个变量之间的关系
    最小二乘法 least square 是使拟合函数与数据之间的均方误差 达到最小的拟合方法
    
    问题:
    设有一个数据序列 X， 通过X的一个函数来预测另一个数据序列Y，如果这个预测函数是线性的，
    截距为 alpha, 斜率 beta, 那么我们可以预期 y_i 大于会等于 alpha + beta*x_i
    除非这两序列是完全线性相关的，否则我们只能近似预测Y的值，预测的离差(或称为 残差) 为
        c_i = (alpha+beta*x_i) - y_i
    残差的出现可能是由数据测量误差造成的，也可能是一些我们未知的非随机因素引起的，比如用身高预测体重时，未知因素可能是饮食，身体锻炼，体型等
    
    如何使残差的平方和最小？ 最常用的方法是
        min(alpha,beta)∑(c_i)**2
    以上选择的原因包括:
        1，平方能将正残差和负残差都变成正数，这符号我们的目标
        2，平方相当于给残差赋予一个权重，越大的残差(绝对量)被赋予的权重越大。但是并不是所以情况下大的残差都应该被赋予大的权重，因为这样拟合方程很容易受到异常值的影响
        3，残差服从均值为0，方法sigma**2（未知，但为常数）的正态分布，且在残差与x独立的假设下，参数的最小二乘估计结果与极大似然估计量相同
        4，最小二乘估计的计算方便简单
    最小二乘不一定总是最好选择
    用X预测Y，如果预测结果偏高造成的代价远低于预测偏低的代价，那么构造一个损失函数cost(c_i)，并最小化损失函数会是更好的选择
    最小二乘拟合的步骤:
        1,计算两个序列的均值 x^ 和 y^ ，X的方差，X和Y的协方差
        2,估计斜率
            beta = Cov(X,Y)/Var(X)
        3,估计截距
            alpha^ = y^ - beta^ * x^   #请另外维基百科了解公式的推导
    
    编写一个LeastSquares的函数，估计X和Y的回归系数 alpha^ 和 beta^, 参数代码:correlation.py
    在BRFSS数据中，用身高对体重的对数进行最小二乘拟合，可以从 brfss_corr.py 参考代码
    
    问题挑战:
        某个地点风速分布决定了该地点风能密度。(风能密度是安装在该位置的风力发动机产能的平均功率上限)，根据经验，风速经验分布接近与 威布尔分布
        
        问题是在测量风速的尾巴时很难，不大可能在一次试验中被观测到
        
        解决办法1: 估计威布尔分布的参数，再对分布积分计算风能密度
        提示: 习题4-6， 
        1，威布尔分布cdf(x) = 1 - e**(-(x/lambda)**k),找到将威布尔分布变成一条直线的 方法
        这条直线的斜率和截距分别表示什么意思？ python中 random.weibullvariate可以生成一个服从威布尔分布的样本
        2，使用最小二乘拟合 来计算变换数据的斜率和截距
        3，编写一个从威布尔分布中抽取样本并估计分布参数的 函数
        4，编写一个利用威布尔分布参数计算平均风能密度的函数(需要风力方面的专业知识)
    
## 拟合优度 - 确定系数
    用线性模型拟合完数据后，我们需要评估模型拟合的好坏情况，当然这种评估取决与用模型来做什么。一种评估模型的办法是计算模型预测能力
    因变量 dependent variable: 在一个预测模型中，我们要预测的值
    自变量 independent variable/解释变量 explanatory variable: 用于预测的值
    
    通过计算模型的 确定系数 coefficient of determination 来评价模型的预测能力，也是通常说的
    R**2 = 1 - Var(c) / Var(Y)
    
    确定系数 的例子:
        我打算猜测一群人的体重是多少，你知道这群人的平均体重是 y^. 如果除此之外你对这些人一点儿都不了解，那么你最佳的策略是选择猜测他们所有人的体重都是 y^
        这时，估计的均方误差就是这个群体的方差 Var(Y)
            MSE = (1/n)∑(y^ - y_i)**2 = Var(Y)
        如果现在又知道来这些人的身高信息，那么你就可以猜测体重大约为 alpha^ + x_i* beta^, 这时，估计的均方误差就为
            Var(c) = MSE = (1/n)∑(alpha^ + x_i* beta^ - y_i)**2 
        
         Var(c) / Var(Y) 表示的是有解释变量情况下的均方误差与没有解释变量情况下的均方误差的比值，也即不能被模型解释的均方误差占总的均方误差的比例
         这样R**2 表示的就是能被模型解释的变异性的比例
         
         假设一个模型的R**2 = 0.64， 那么我们就可以说这个模型解释来 64%的变异性，或精确的说，这个模型使你预测的均方误差降低了64%
    
    在最小二乘模型中，我们可以证明确定系数和两个变量的皮尔逊相关系数存在一个非常简单的关系即 R**2 = p**2
    查阅资料了解其证明过程
    
    问题挑战:
        编写一个Residuals的函数，根据X,Y,alpha^ 和 beta^ 计算残差 ci
        编写 coefDetermination函数，根据 c_i 和 Y 计算 R**2 要测试函数的正确性 看是否有R**2 = p**2
        参考代码: correlation.py
    
    挑战2:
        根据BRFSS身高和体重的数据，计算 alpha^ beta^ 和 R**2. 如果需要预测一个人的体重，它的身高会对你起多大的积极作用?
        参考代码 brfss_corr.py
        

## 相关性和因果关系
    两个变量的相关关系不能告诉我们 变量的变化是否由另一个变量变化引起。
    或许变量之间就没有因果关系，而是因为其他原因导致二者同时发生相同或相反趋势
    也可能是偶然因素造成他们之间的相关性(xkcd.com/552)
    需要得证  相关信号能导致因果关系
        1， 利用时间先后关系。如果事件 A 在B之前发生，那么A就有可能是导致B的原因( 反之不成立)
            事件发生顺序可以帮助我们推断因果顺序，但是这并不能排除存在另外一些事件导致A和B的发生
        2， 利用随机性，如果我们将一个非常大的总体随机分成两部分，然后分别计算这些变量在两个子总体平均值
            我们可以期望这些均值差异很小，因为中心极限定理可以保证
            如果一个变量在两个分组中有明显差别，其他所有变量在两个分组里都几乎一样，这时我们就可以排除一些虚假相关性
            即使相关变量未知，这个方法也可行，如果知道相关变量，我们就能检查两个分组是否一致
    
    随机对照试验 randomized controlled trial 就是根据这些想法设置的。它是 因果关系鉴定的最可信赖方法之一
    这种试验中棉被试者随机分成两组或多组
    实验组，接受干预， 对照组 不接受干预，或只接受已知效应的处理
    缺点: 只能用于实验研究，药物研发少数情况
    
    自然试验 natural experiment
    我们尽量控制群体在各个方面都是相似的，然后对不同群体实施不同处理
    缺点:各群体可能有一些我们观测不到的差异
    
    回归分析 regression analysis 推断因果关系
    线性最小二乘是用自变量 解释 因变量 的简单回归，有一个自变量和多个自变量的区别
    
    以上方法都是用于控制虚假的相关性
    
    问题挑战:
        NSFG数据包含一个记录婴儿出生时母亲年龄变量 agepreg，绘制母亲年龄和新生儿体重的散点图，观测两者关系
        
        用母亲年龄对新生儿出生体重进行最小二乘拟合。 估计量 alpha^  beta^ 单位是什么？ 是否可以用一两句话描述拟合的意义
        
        计算头胎的 母亲年龄均值 和 非头胎 母亲年龄均值。两个分组的母亲年龄均值差异有多大时会影响新生儿出生体重差异? 新生儿体重差异有多大比例可以用 母亲年龄差异来解释?
        
        参考代码: agemodel.py
        多元回归代码: age_lm.py  通过py 调用 R统计计算软件
        
    
      