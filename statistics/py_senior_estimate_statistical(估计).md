#估计
    有些时候经常用样本均值近似替代分布均值，比如我们知道有样本为[1,2,3,4,5],那么样本均值为3
    我们有理由认为分布的均值也为3
    这个过程叫估计
    
## 估计量和最小化均方误差MSE
    用来估计分布参数的统计量，如样本均值，称为估计量
    
    一般先对样本进行修剪，然后用剩下的样本均值来做估计参数
    或者使用中位数 来作为估计量
    
    选择那种方式取决于 我们想让估计误差最小，还是让得出正确答案的可能性最大
    
    假设样本不存在异常值，那么样本均值会最小化均方误差 MSE(Mean Squared Error)
    MSE = (1/m)𝞢(x - mu)**2
    m表示估计次数， n 表示每次估计得到的样本数量，x为样本均值，mu 为分布均值
    使用样本均值 估计 mu能最小化均方误差

## 极大似然估计量 MLE(Maximum Likelihood Estimator)
    以最大概率猜对的估计
    例: 抛掷3次6面骰子，然后问你3次得到的点数总和是多少，猜对有奖
    如果使用均方误差 10.5，但是这个并不是可以得到的真正的数字
    如果选择10或11，将有1/8机会猜对，这才是 MLE   
    
    分别使用平均值和中位数作为 分布均值的估计，再计算此时的均方误差，比较两种方式的异同。
    
## 方差估计 与有偏估计
    有样本如下 L= [-0.441, 1.774, -0.101, -1.138, 2.975, -2.138]
    L的样本方差 S2 = (1/n)𝞢(xi - x)**2
    分布方差sigma2, 如果样本数量很少， S2 低估与 实际的分布方差sigma2
    此时S2 只是 sigma2的 有偏估计
    
### 无偏估计 unbiased
    如果进行很多次估计后，我们发现估计量和真实参数的误差的平均值为0，那么这个估计量就是无偏的(unbiased)
    sigma**2的一个无偏估计 (S_n-1)**2 = (1/n-1)𝞢(xi-x)**2
    
    样本均值可以是(S_n)**2 也可以是(S_n-1)**2 # 注意区别

### 误差
    均方差和有偏性都是长期概念，是多次进行试验后得到的结果
    误差是为了描述不同的估计量在多次试验下的表现。
    在试验中，参数都是人工设置的，已知的，所以我们可以计算误差。
    * 但是实际数据中，我们并不知道参数的真实情况，也就无法得到误差值。
    
### 指数分布的估计
    有一组样本[5.384, 4.493, 19.198, 2.790, 6.122, 12.844]
    这个指数分布样本参数lambda是多少呢?
    指数分布均值为 1/lambda
    一般情况下，lambda = 1/x  (x为样本均值)
    我们给待估的参数一个帽子，以此表示参数的估计量，_lambda 不仅是 lambda的估计量，而且还是它的极大似然估计量，
    所以如果我们要有最大可能性猜对lambda， _lambda是最好选择
    
    可以用另一种基于中位数的方法估计lambda，我们知道指数分布的中位数等于log2(x)/lambda, 
    定义lambda的估计量为 _lambda_1/2 = log_2(2)/mu_1/2
    mu_1/2 是样本中位数
    
    练习:模拟估计 _lambda 和 _lambda_1/2 哪个更低，他们是否有偏？
    
### 置信区间
    特别的 点估计(point estimation): 用估计量产生一个值来估计参数
    
    更一般的，我们想知道整个分布的情况，也就是分布参数所有取值范围，在此范围，每个值的可能性
    置信区间 通常通过缺失率 miss rate 来描述，90%置信区间对应的 alpha=0.1，分布的参数lambda置信区间为
    (_lambda*(x**2(2n, 1-a/2))/2n, _lambda*(x**2(2n,alpha/2))/2n)
    n表示样本数量，_lambda是上面提到的参数的均值估计， x**2(k,x)是自由度为k的卡方分布的累计分布函数在x处的值
    
    * 一般来说，很难用分析的方法推导出参数的置信区间，但是用模拟的形式来估计相对容易很多    
    
### 贝叶斯估计
    有一组样本: L2= [2.675, 0.198, 1.152, 0.787, 2.717, 4.269]
    L2的lambda值是多少？ lambda是一个随机变量，先确定它服从什么分布，使用贝叶斯很容易计算
    贝叶斯步骤:
    1,将(0.5,1.5)划分为一组长度相等的小区间，每个小区间我们定义假设H_i 为: lambda落在第i区间，因为lambda服从均匀分布
    所以H-i的先验概率 P(H_i)对所有i都是相等的
    2，对每一个假设H_i, 我们计算似然函数P(x|H_i)，即在H_i条件下出现样本X的概率为
        P(X|H_i) =  ∏(j) * expo(lambda_i, x_j)
    expo(lambda_i, x_j) 表示参数为lambda的指数分布在x处的概率密度函数 
        PDF_expo(lambda,x) = lambda*e**-lambda*x
    3,然后利用贝叶斯定理计算后验概率
        P(H_i|X) = P(H_i)*P(X|H_i)/ f
        f是一个归一化因子
        f=𝞢(i)P(H_i)P(X|H_i)
    得到参数的后验分布后美酒很容易计算置信区间来。90%的置信区间上下限就可以选择后移分布95%个5%百分位数
    
    贝叶斯置信区间有时又称为 可信区间(credible interval)
    贝叶斯统计定义的置信区间与频率学派定义的置信区间有差别
    删失数据censored data，有些数据被系统性地排除在外了
    优点:处理删失数据相对容易。
        
### 贝叶斯估计的实现
    Pmf和Cdf表示分布的函数来表示先验，因为我们要将假设映射 为概率，所以Pmf是更好的选择
    贝叶斯估计的实现 代码实现 参考 code estimate
        生成来lambda先验的Pmf，Pmf所有取值 构成来我们所有的假设，我们把这个假设的集合称为一个suite。
        所有假设有相同概率，所以这个Pmf是一个均匀分布
    贝叶斯估计的理解方法
        想象有一房子人在猜测我想要估计的值，假设初始值为 lambda1
        起初房子的人对每一个猜测值都有一个置信度，相信有多大的可能性猜对
        当有新的证据时，所有人都根据P(E|H)更新自己的置信度，这里P(E|H)是指起初猜测正确的假设下，E的似然值
        
        通常似然函数会计算出概率，最大为1，一开始每个人的置信度会下降或保持不变，但当结果归一化后，置信度又会上升。
        上述过程 净效应就是 有些人置信度上升了，有些人下降了，这取决于他们的 相对似然值
        
    例: 火车头的估计问题
    铁路公司将所有火车头都进行了编号，从1到N，有一天你看见一个编号 100的车头，该铁路公司总共有多次火车头？    
    当观测值增多时，贝叶斯估计趋向与收敛
    
    
    
